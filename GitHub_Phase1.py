'''
This script queries for 1000 repositories pushed in the last year. 
This 1000 is a configurable parameter which can be changed from the Config.json file by changing the variable max_size. 
These repo's are sorted from most to least stars. 
Requires:   A configuration file called Config.json
            Ensure a GitHub token generated by your account is in the configuration account so that the script may connect to github 
            
Output:     A text file called Top_repo.txt which has all the 1000 repository full names (which will be used in GitHub_Phase2.py)
'''
import sys
import os

import datetime
import random
import csv
import traceback
from github import Github, GithubException
from datetime import date
import Common_Utilities

#Outputs all the repositories found into a text file
def output_to_file(repos_file, repo_set):
    print("Writing ", len(repo_set), " repos to file")
    with open(repos_file, "a") as csv_file:
        writer = csv.writer(csv_file)
        writer.writerow(["Full name", "URL", "Stars", "Num contributors", "Last updated"])
        for k, v in repo_set.items():
            # print(k, v)
            writer.writerow([k, v[0], v[1], v[2], v[3]])

             
# This is where we query for the top repositories
def query_repo(output_file_name, base_query, github, quick_sleep, error_sleep, max_size):
    
    repo_set = {}
    try: #check github for rate limit 
        rate_limit = github.get_rate_limit()
        rate = rate_limit.search
        print("The rate limit is %d" % rate.limit)
    
        if rate.remaining == 0:
            print('You have 0/%d API calls remianing. Reset time: %d' % (rate.limit, rate.reset ))
            Common_Utilities.go_to_sleep("Reached API limit per minute, Going to sleep for ", quick_sleep) 
        else:
            print('You have %d/%d API calls remaining' % (rate.remaining,rate.limit))
      
        print ('Base query: %s' % base_query)
        curr_query = base_query + " stars:>=50 fork:false archived:false"

            
           
        while len(repo_set) < max_size:
            print ("Collected ", len(repo_set), " repos so far")
            print (curr_query)
            result = github.search_repositories(curr_query, sort='stars', order='desc')
            cnt = 0
            pgno = 1
            
            # 300 is how many repo's the script reads at a time (it was kept at 300 as reading more than that may result in a crash of the Github object 
            while cnt <= 300:
                try:          
                    for repo in result.get_page(pgno):
                        num_contributors = repo.get_contributors().totalCount
                        if num_contributors < 3:
                            continue
                        repo_info = {repo.full_name: []}
                        repo_info[repo.full_name].append(repo.html_url)
                        repo_info[repo.full_name].append(repo.stargazers_count)
                        repo_info[repo.full_name].append(num_contributors)
                        repo_info[repo.full_name].append(repo.updated_at.isoformat())

                        # update repo_set
                        repo_set.update(repo_info)
                        # clear the temp repo_infor before the next iteration
                        repo_info.clear()

                        cnt = cnt + 1
          
                        stars = repo.stargazers_count

                        if len(repo_set) == max_size:
                            break
                    print("cnt = " + str(cnt))
                except Exception as e:
                    print(e)
                    Common_Utilities.go_to_sleep("API limit exceeded, Going to sleep for ", quick_sleep)
                    continue

                if len(repo_set) == max_size:
                    break

                pgno = pgno + 1  
            
            curr_query =  base_query + " fork:false archived:false stars:50.." + str(stars)
            
        # print(repo_set)
        output_to_file(output_file_name, repo_set)

    # error detection, just in case
    except Exception as e:
        output_to_file(output_file_name, repo_set)
        print("Error: abuse detection mechanism detected.. outputting what we have...")
        traceback.print_exc()

#Main function where we set the variables from the configuration file and connect to github 
def get_top_repos():

    print("Retrieving list of top repos... \n")
    
    config_dict = Common_Utilities.read_config_file() # read all ini data    
    
    #find top repos that have pushed in the last X days
    time_span = int(config_dict["TIME_SPAN"]) 
    push_date = date.today() - datetime.timedelta(days=time_span)
    
    quick_sleep = int (config_dict["QUICK_SLEEP"]) # regular sleep after each iteration
    error_sleep = int (config_dict["ERROR_SLEEP"]) # Sleep after a serious issue is detected from gitHib, should be around 10min, ie 600 sec
    max_size = int (config_dict["MAXSIZE"]) # maximum number of repos to look for (top X); configured in config file
    
    github = None
    github = Github(config_dict["TOKEN"])   # pass the connection token 
    
    output_file_name = "Python_Projs.csv"  # this is the output file that we are going to send repo names to
    
    output_file = open(output_file_name, "w")  
    output_file.close()      
    
    query = "machine learning pushed:>" + str(push_date) + " " + config_dict["REPO_SEARCH_QUERY"]
    print (query)
   
    query_repo(output_file_name, query, github, quick_sleep, error_sleep, max_size)
             
    print ("\nFinally ..... Execution is over \n")
      
if __name__ == "__main__":
    get_top_repos()
